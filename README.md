<p align="center">
  <img src="visuals/lab_banner.svg"
       alt="Enlightened AI Research Lab Banner"
       width="100%" />
</p>

<p align="center">
  <a href="https://github.com/EnlightenedAI-Lab">
    <img src="https://img.shields.io/badge/Org-EnlightenedAI--Lab-black?logo=github" />
  </a>
  <img src="https://img.shields.io/badge/focus-reflective%20alignment-blue" />
  <img src="https://img.shields.io/badge/location-Montr%C3%A9al%2C%20Canada-lightgrey" />
  <a href="https://www.enlightenedai.ai">
    <img src="https://img.shields.io/badge/site-enlightenedai.ai-orange" />
  </a>
</p>

---

## ğŸ”¬ Research Focus

### Reflective Alignment Architecture (RAA)

A multi-layer framework (L1â€“L7) designed to measure:

- Internal coherence  
- Value alignment  
- Longitudinal drift  
- Reflective self-correction  
- System-level stability  

### Reflective Duality Layer (RDL)

Reflective signal layer of RAA, comparing:

- Forward answer  
- Reflective answer  
- Î” improvement (Râˆ‡)  
- Coherence stability (MCIâ˜…)  

### Applied Evaluation Tools

We develop:

- **LLM-Judge** â€“ evaluation engine for coherence, stability, and drift  
- **MIRROR-H** â€“ Humanâ€“AIâ€“Earth coherence model and interpretability lens  
- **RAA-Core** â€“ formal theory, stability definitions, and conceptual diagrams  

---

## ğŸ“‚ Key Projects

(See `PROJECTS.md` for the full list.)

- **LLM-Judge**  
  Multi-layer evaluation system for reflective coherence, stability, and drift.  
  <https://github.com/EnlightenedAI-Lab/LLM-Judge>

- **RAA-Core**  
  Formal theory and diagrams of the Reflective Alignment Architecture.  
  <https://github.com/EnlightenedAI-Lab/RAA-Core>

- **MIRROR-H**  
  Humanâ€“AIâ€“Earth stability framework and interpretability toolkit.  
  <https://github.com/EnlightenedAI-Lab/Mirror-H>

---

## ğŸ“š Diagram Library

All diagrams and conceptual figures are consolidated in:

â¡ï¸ **[Diagram Gallery](DIAGRAMS.md)**  

(Links out to per-diagram pages with captions and explanations.)

---

## ğŸŒ Contact

**Website:** <https://www.enlightenedai.ai>  
**Email:** research@enlightenedai.ai  
**GitHub Org:** <https://github.com/EnlightenedAI-Lab>  
**Location:** MontrÃ©al, Canada  
**ORCID:** <https://orcid.org/0009-0006-5352-9727>  

---

## ğŸ¤ Funders & Collaborators

We welcome collaboration with research labs, foundations, and institutions interested in:

- Reflective alignment & stabilizing frontier AI systems  
- Long-horizon behavioural prediction  
- Safety evaluation frameworks  
- Scientific foundations of AI coherence  

> If you would like to collaborate or support this work, please contact  
> **research@enlightenedai.ai**

---

## ğŸ“œ License

All repositories under Enlightened AI Research Lab are released with open scientific intent.  
License terms may differ per project.
