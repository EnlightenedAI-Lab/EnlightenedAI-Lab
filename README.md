<p align="center">
  <img src="./visuals/lab_banner.svg"
       alt="Enlightened AI Research Lab Banner"
       width="100%" />
</p>

<p align="center">
  <a href="https://github.com/EnlightenedAI-Lab">
    <img src="https://img.shields.io/badge/Org-EnlightenedAI--Lab-black?logo=github" />
  </a>
  <img src="https://img.shields.io/badge/focus-reflective%20alignment-blue" />
  <img src="https://img.shields.io/badge/location-Montr%C3%A9al%2C%20Canada-lightgrey" />
  <a href="https://www.enlightenedai.ai">
    <img src="https://img.shields.io/badge/site-enlightenedai.ai-orange" />
  </a>
</p>

---

## ğŸ”¬ Research Focus

### **Reflective Alignment Architecture (RAA)**
A multi-layer framework (L1â€“L7) designed to measure:
- Internal coherence  
- Value alignment  
- Longitudinal drift  
- Reflective self-correction  
- System-level stability  

### **Reflective Duality Layer (RDL)**
The mathematical core of RAA.  
Compares:
- Forward answer  
- Reflective answer  
- Î” improvement (Râˆ‡)  
- Coherence stability (MCIâ˜…)  

### **Applied Evaluation Tools**
We develop:
- LLM-Judge (evaluation engine)
- Mirror-H (semantic and interpretability lens)
- RAA-Core (formal definitions, equations, diagrams)

---

## ğŸ“‚ Key Projects

See **PROJECTS.md** for full list.

- **LLM-Judge**  
  Multi-layer evaluation system for reflective coherence, stability, and drift.  
  https://github.com/EnlightenedAI-Lab/LLM-Judge

- **RAA-Core**  
  Formal definitions, math, and diagrams of the Reflective Alignment Architecture.  
  https://github.com/EnlightenedAI-Lab/RAA-Core

- **Mirror-H**  
  Interpretability toolkit for semantic stability, gyroscopic alignment, and reflective mapping.  
  https://github.com/EnlightenedAI-Lab/Mirror-H

---

## ğŸ“š Diagram Library

All diagrams and conceptual figures used in the Reflective Alignment Architecture (RAA) are consolidated here:

â¡ï¸ **[DIAGRAMS.md](diagrams/DIAGRAMS.md)**

This index will include thumbnails, descriptions, and links to all diagrams once the full upload is complete.

---

## ğŸŒ Contact

**Website**: https://www.enlightenedai.ai  
**Email**: research@enlightenedai.ai  
**GitHub Organization**: https://github.com/EnlightenedAI-Lab  
**Location**: Montreal, Canada  
**ORCID**: https://orcid.org/0009-0006-5352-9727  

---

## Funders & Collaborators

We welcome collaboration with research labs, foundations, and institutions interested in:

- Reflective alignment & stabilizing frontier AI systems  
- Long-horizon behavioural prediction  
- Safety evaluation frameworks  
- Scientific foundations of AI coherence  

> **If you would like to collaborate or support this work, please contact:**  
> **research@enlightenedai.ai**

Your name or institution could be listed here.

---

## ğŸ“œ License
All repositories under Enlightened AI Lab are released with open scientific intent.  
License terms may differ per project.
